{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03 - Logistic regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VxCeEJ9mjZuC","colab_type":"text"},"source":["# Logistic Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gEndPDUNEe-H","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dRqCpkpdMdTQ"},"source":["## Make a dataset for Logistic Regression\n","\n","### Logistic Regression을 위한 Dataset을 임의로 만들어 봅시다.\n","\n","* 2가지 위치에 몰려있는 데이터\n","* 테스트를 위한 빨간색 데이터"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9yre_ERVEma8","colab":{}},"source":["x_train = [[1., 1.2],\n","          [2., 1.3],\n","          [3., 1.1],\n","          [4., 2.9],\n","          [5., 2.7],\n","          [6., 2.8]]\n","y_train = [[0.],\n","          [0.],\n","          [0.],\n","          [1.],\n","          [1.],\n","          [1.]]\n","\n","x_test = [[3.5,2.15]]\n","y_test = [[1.]]\n","\n","x1 = [x[0] for x in x_train]\n","x2 = [x[1] for x in x_train]\n","\n","colors = [int(y[0] % 3) for y in y_train]\n","plt.scatter(x1,x2, c=colors , marker='^')\n","plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n","\n","plt.xlabel(\"x1\")\n","plt.ylabel(\"x2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hYtiGWxjZuJ","colab_type":"text"},"source":["## tf.data.Dataset\n","* 데이터를 관리해주기위한 tf function\n","* 각 데이터의 필요 기능들을 지원해준다.\n","* 데이터셋 크기가 클 경우에 메모리에 나눠올리는 기능을 지원"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MxddblPApI8v","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices(\n","    (x_train, y_train)).batch(len(x_train))\n","\n","W = tf.Variable(tf.random.normal([2,1]), name='weight')\n","b = tf.Variable(tf.random.normal([1]), name='bias')\n","\n","tf.print(W, b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BYPKT6To56Iv"},"source":["## Sigmoid 함수를 가설로 선언합니다\n","* Sigmoid는 아래 그래프와 같이 0과 1의 값만을 리턴합니다 tf.sigmoid(tf.matmul(X, W) + b)와 같습니다\n","\n","## $$\n","\\begin{align}\n","sigmoid(x) & = \\frac{1}{1+e^{-x}}  \\\\\\\\\\\n","\\end{align}\n","$$\n","\n","![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LFAWvdar8PP-","colab":{}},"source":["def logistic_regression(features):\n","    hypothesis  = tf.divide(1., 1. + tf.exp(-(tf.matmul(features, W) + b)))\n","    return hypothesis\n","\n","tf.print(logistic_regression(x_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PXHVNZ6W8V83"},"source":["## 가설을 검증할 Cost 함수를 정의합니다\n","$$\n","\\begin{align}\n","cost(h(x),y) & = −log(h(x))  &  if :  &  y=1 \\\\\\\\\\\n","cost(h(x),y) & = -log(1−h(x))  &  if :  &  y=0\n","\\end{align}\n","$$\n","\n","### 두 식을 한번에 쓰게되면,\n","\n","$$\n","\\begin{align}\n","cost(h(x),y) & = −y log(h(x))−(1−y)log(1−h(x))\n","\\end{align}\n","$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oxIgE6nt8zvR","colab":{}},"source":["def loss_fn(hypothesis, labels):\n","    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + \\\n","                           (1 - labels) * tf.math.log(1 - hypothesis))\n","    return cost\n","\n","optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8xmYMVq65vFd","colab":{}},"source":["epochs = 5000\n","\n","for step in range(epochs):\n","  for features, labels in dataset:\n","    with tf.GradientTape() as tape:\n","      loss_value = loss_fn(logistic_regression(features),labels)\n","      grads = tape.gradient(loss_value, [W,b])\n","      optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n","      if step % 100 == 0:\n","            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),labels)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ci-nHblj--z9","colab":{}},"source":["def accuracy_fn(hypothesis, labels):\n","    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YANtydPI6-sl","colab":{}},"source":["test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n","print(\"Testset Accuracy: {:.4f}\".format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Joo_P-vtjZud","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}