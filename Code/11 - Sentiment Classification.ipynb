{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"11 - Sentiment Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"9JutkdoS4X-A","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","\n","import time\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9BlQYqMaBud","colab_type":"text"},"source":["# Word 감정분석\n","* Text를 모델에 학습시키기 위한 전처리 과정을 확인해보자"]},{"cell_type":"markdown","metadata":{"id":"raI-Z3pJdc8y","colab_type":"text"},"source":["### 데이터셋 구성\n","* 각 단어들을 정의 (긍정과 부정을 나타내는 단어들)\n","* 단어에 대한 정답을 부정(0) 긍정(1)로 정의해서 데이터셋 구성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oAN3lQTI_8Ef","colab":{}},"source":["x_train_words = ['good', 'bad', 'amazing', 'so good', 'bull shit',\n","                 'awesome', 'how dare', 'very much', 'nice', 'god damn it',\n","                 'very very very happy', 'what the fuck']\n","y_train = np.array([1, 0, 1, 1, 0,\n","                    1, 0, 1, 1, 0,\n","                    1, 0], dtype=np.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnuyhzN5eT_Q","colab_type":"text"},"source":["* 데이터셋 확인"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FORaXQQf_862","colab":{}},"source":["# negative sample\n","index = 0\n","print(\"word: {}\\nlabel: {}\".format(x_train_words[index], y_train[index]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YM_HvHb6_-Ae","colab":{}},"source":["# positive sample\n","index = 1\n","print(\"word: {}\\nlabel: {}\".format(x_train_words[index], y_train[index]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IgSmU_ZBeV2u","colab_type":"text"},"source":["### 텍스트데이터 처리를 위한 Tokenizer 사용"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2kRKkAVK_50f","colab":{}},"source":["from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkMHz1YWeb2M","colab_type":"text"},"source":["* 캐릭터(알파벳) 단위로 단어를 자르는 Tokenizer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2noBANyM_-r2","colab":{}},"source":["tokenizer = Tokenizer(char_level=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SklZllNuAA0f","colab":{}},"source":["tokenizer.fit_on_texts(x_train_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pxkPyVbwACIm","colab":{}},"source":["num_chars = len(tokenizer.word_index) + 1\n","print(\"number of characters: {}\".format(num_chars))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QL7h-uqWe0-8","colab_type":"text"},"source":["* 각 캐릭터(알파벳)의 index값"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k-fGFTKFAC-e","colab":{}},"source":["tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tf9buNr_e9Xe","colab_type":"text"},"source":["* 학습을 위한 train word를 idx로 변환"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iD5DPYtqAD0m","colab":{}},"source":["x_train_tokens = tokenizer.texts_to_sequences(x_train_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UDLR11MXAEvG","colab":{}},"source":["index = 2\n","print(\"text: {}\".format(x_train_words[index]))\n","print(\"token: {}\".format(x_train_tokens[index]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1C_SYFvrAFhG","colab":{}},"source":["x_train_seq_length = np.array([len(tokens) for tokens in x_train_tokens], dtype=np.int32)\n","num_seq_length = x_train_seq_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-MA7qyvgMjz","colab_type":"text"},"source":["* Input 데이터의 길이는 모두 같아야 함으로 최대 길이에 맞춰 길이를 조정해준다."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RSzI90d1AGWu","colab":{}},"source":["max_seq_length = np.max(num_seq_length)\n","print(max_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x9CbLSdDAHMO","colab":{}},"source":["pad = 'pre'\n","# pad = 'post'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8aplGSLhAIPf","colab":{}},"source":["x_train_pad = pad_sequences(sequences=x_train_tokens, maxlen=max_seq_length,\n","                            padding=pad, truncating=pad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N4C03AUxgUnW","colab_type":"text"},"source":["* 학습을 위해 처리된 데이터를 확인해보자"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9FiKxkqKAJGu","colab":{}},"source":["index = 7\n","print(\"text: {}\\n\".format(x_train_words[index]))\n","print(\"token: {}\\n\".format(x_train_tokens[index]))\n","print(\"pad: {}\".format(x_train_pad[index]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"atI8XdnGgjl5","colab_type":"text"},"source":["### 토큰이 어떻게 처리되었는지 확인해보자"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ih2hW7FKAJ4H","colab":{}},"source":["idx = tokenizer.word_index\n","inverse_map = dict(zip(idx.values(), idx.keys()))\n","print(inverse_map)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OpR9VCNBAKzW","colab":{}},"source":["def tokens_to_string(tokens):\n","  # Map from tokens back to words.\n","  words = [inverse_map[token] for token in tokens if token != 0]\n","\n","  # Concatenate all words.\n","  text = \"\".join(words)\n","\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZjY3jPJNALln","colab":{}},"source":["index = 10\n","print(\"original text:\\n{}\\n\".format(x_train_words[index]))\n","print(\"tokens:\\n{}\\n\".format(x_train_tokens[index]))\n","print(\"tokens to string:\\n{}\".format(tokens_to_string(x_train_tokens[index])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TI72PhzQgmxh","colab_type":"text"},"source":["### 데이터셋 구성\n","* 데이터 학습을 위한 데이터셋 구성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OxSoUVVPAMVu","colab":{}},"source":["# Set the hyperparameter set\n","batch_size = 4\n","max_epochs = 50\n","# embedding_size = 8\n","num_units = 16 # the number of nodes in RNN hidden layer\n","num_classes = 2 # Two classes [True, False]\n","initializer_scale = 0.1\n","learning_rate = 1e-3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Vn3VTe7rANH3","colab":{}},"source":["## create data pipeline with tf.data\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train_pad, x_train_seq_length, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size = 100)\n","train_dataset = train_dataset.repeat()\n","train_dataset = train_dataset.batch(batch_size = batch_size)\n","print(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YMeIdfv-AOCW","colab":{}},"source":["model = tf.keras.Sequential([\n","          layers.Embedding(num_chars, num_chars, embeddings_initializer='identity', trainable=False),\n","          layers.SimpleRNN(units=num_units),\n","          layers.Dense(units=num_classes, activation='sigmoid')])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5CIg4EF5AOsv","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate)\n","loss_obj = tf.keras.losses.BinaryCrossentropy(logits=False)\n","mean_loss = tf.keras.metrics.Mean(\"loss\")\n","loss_history = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fz4HRQEtgrA6","colab_type":"text"},"source":["### 학습 진행\n","* tf.GradientTape을 이용한 학습 진행"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GD5R3wLmAPc2","colab":{}},"source":["total_steps = int( len(x_train_words) / batch_size * max_epochs)\n","for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset.take(total_steps)): # just steps number (iterations), NOT epochs\n","  start_time = time.time()\n","  with tf.GradientTape() as tape:\n","    logits = model(seq_pad)    \n","    loss_value = loss_obj(tf.one_hot(labels, depth=num_classes), logits)\n","\n","  mean_loss(loss_value)\n","  loss_history.append((mean_loss.result().numpy()))\n","  grads = tape.gradient(loss_value, model.variables)\n","  optimizer.apply_gradients(zip(grads, model.variables))\n","  \n","  if step % 3 == 0:\n","      clear_output(wait=True)\n","      duration = time.time() - start_time\n","      examples_per_sec = batch_size / float(duration)\n","      epochs = batch_size * step / float(len(x_train_words))\n","      print(\"epochs: {:.2f}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs+1, step, loss_value, examples_per_sec, duration))\n","    \n","print(\"training done!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iFLKQz4pAQI3","colab":{}},"source":["loss_history = np.array(loss_history)\n","plt.plot(loss_history, label='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ltoEOXq-g5KW","colab_type":"text"},"source":["### 모델 평가\n","* 작은 데이터셋 이므로 train_set으로 다시 평가해보자"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eyZ_uhW4AbEv","colab":{}},"source":["train_dataset_eval = tf.data.Dataset.from_tensor_slices((x_train_pad, x_train_seq_length, y_train))\n","train_dataset_eval = train_dataset_eval.batch(batch_size = len(x_train_pad))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ctDm0BweAbyH","colab":{}},"source":["# accuracy = tf.keras.metrics.CategoricalAccuracy()\n","\n","# for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset.take(1)):\n","#   logits = model(seq_pad)\n","# #   accuracy(labels=labels, prdictions=tf.cast(tf.argmax(logits, 1), tf.int32))\n","#   mean_accuracy = tf.keras.metrics.Mean(\"accuracy\")\n","  \n","# print(\"test accuracy: {}\".format(accuracy.result()))\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","acc_object = tf.keras.metrics.CategoricalAccuracy()\n","val_acc_object = tf.keras.metrics.CategoricalAccuracy()\n","\n","val_mean_loss = tf.keras.metrics.Mean(\"loss\")\n","val_mean_accuracy = tf.keras.metrics.Mean(\"accuracy\")\n","\n","for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset.take(1)):\n","    predictions = model(seq_pad, training=False)\n","    val_loss_value = loss_object(tf.one_hot(labels, depth=num_classes), predictions)\n","    val_acc_value = val_acc_object(tf.one_hot(labels, depth=num_classes), predictions)\n","\n","    val_mean_loss(val_loss_value)\n","    val_mean_accuracy(val_acc_value)\n","\n","    print(\"valid loss: {:.4g}, valid accuracy: {:.4g}%\".format(val_mean_loss.result(),\n","                                                             val_mean_accuracy.result() * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dNB5vDg2AchX","colab":{}},"source":["for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset_eval.take(1)):\n","  logits = model(seq_pad)\n","  predictions = tf.cast(tf.argmax(logits, 1), tf.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QiekIdm-AdL2","colab":{}},"source":["predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQWcMXlrg-67","colab_type":"text"},"source":["* 단어를 입력했을때 모델의 예측값"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OeLILL78AeLP","colab":{}},"source":["for x, y in zip(seq_pad, predictions):\n","  if y.numpy() == 1:\n","    print(\"{} : positive\".format(tokens_to_string(x.numpy())))\n","  else:\n","    print(\"{} : negative\".format(tokens_to_string(x.numpy())))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cVZRpMSPAfBH","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}