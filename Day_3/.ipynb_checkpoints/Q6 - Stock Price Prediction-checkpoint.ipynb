{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction\n",
    "\n",
    "### Task\n",
    "* 모델 및 하이퍼파라미터들을 바꿔가며 accuracy를 높혀 보자\n",
    "* 밑에 제시된 여러가지 시도를 해보자\n",
    "* The main flow of this code is referenced in this [blog](https://medium.com/@aniruddha.choudhury94/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de700bff68)\n",
    "* 주식데이터를 직접 다운 받아서 실제 예측을 해보자\n",
    "  * Train data: 2017년 1월 1일 ~ 2019년 12월 31일 데이터\n",
    "  * Test data: 2019년 1월 1일 ~ 2019년 2월 28일 데이터\n",
    "  * Close price 예측 (baseline은 open price)\n",
    "\n",
    "### Dataset\n",
    "* [Yahoo finance datasets](https://www.imdb.com/interfaces/)\n",
    "* 2-3년간 daily stock price 데이터를 이용하여 미래 한달의 주식가격을 예측\n",
    "\n",
    "### Baseline code\n",
    "* Dataset: train, test로 split\n",
    "* Input data shape: (`batch_size`, `past_day`=60, 1)\n",
    "* Output data shape: (`batch_size`, 1)\n",
    "* Architecture: \n",
    "  * `LSTM` - `Dense`\n",
    "  * [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) 사용\n",
    "* Training\n",
    "  * `model.fit` 사용\n",
    "* Evaluation\n",
    "  * `model.evaluate` 사용 for test dataset\n",
    "\n",
    "### Try some techniques\n",
    "* Change model architectures (Custom model)\n",
    "  * Use another cells (LSTM, GRU, etc.)\n",
    "  * Use dropout layers\n",
    "  * Change the `past_day`\n",
    "* Data augmentation (if possible)\n",
    "* Try Early stopping\n",
    "* Use various features (open, high, low, close prices and volume features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "* We colud download daliy stock price using `fix_yahoo_finance` library.\n",
    "* Some stock ticker symbols [NASDAQ] [link](http://eoddata.com/symbols.aspx)\n",
    "  * `AAPL`: Apple Inc.\n",
    "  * `AMZM`: Amazon.com Inc.\n",
    "  * `GOOG`: Alphabet Class C (Google)\n",
    "  * `MSFT`: Microsoft Corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fix_yahoo_finance as yf\n",
    "\n",
    "dataset = yf.download(tickers='AAPL', start='2017-01-01', end='2019-01-01', auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. Data discretization: Part of data reduction but with particular importance, especially for numerical data\n",
    "2. Data transformation: Normalization.\n",
    "3. Data cleaning: Fill in missing values.\n",
    "4. Data integration: Integration of data files.\n",
    "\n",
    "After the dataset is transformed into a clean dataset, the dataset is divided into training and testing sets so as to evaluate. Creating a data structure with 60 timesteps and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Open'].plot(figsize=(16, 6))\n",
    "dataset.Close.plot(figsize=(16, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column to float type when column type is an object\n",
    "#dataset[\"Close\"] = dataset[\"Close\"].str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 day rolling mean\n",
    "dataset.rolling(7).mean().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Close: 30 Day Mean'] = dataset.Close.rolling(window=30).mean()\n",
    "dataset[['Close', 'Close: 30 Day Mean']].plot(figsize=(16, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['Open']\n",
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "train_data_scaled = sc.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(train_data_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "past_days = 60\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(past_days, len(train_data_scaled)):\n",
    "  X_train.append(train_data_scaled[i-past_days:i, 0])\n",
    "  y_train.append(train_data_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Cast\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict the price of next day given past 60 days prices\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(layers.LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(layers.LSTM(units = 50)\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(layers.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model\n",
    "model(X_train[0:2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Test-Set\n",
    "\n",
    "Now that the model has been trained we can calculate its mean squared error on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "# Getting the real stock price of 2018\n",
    "dataset_test = yf.download(tickers='GOOG', start='2019-01-01', end='2019-02-01', auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset_test['Open']\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "test_set_scaled = sc.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_scaled = pd.DataFrame(test_set_scaled)\n",
    "test_set_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2018\n",
    "dataset_total = pd.concat((dataset['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - past_days:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "prediction_days = dataset_test.shape[0]\n",
    "for i in range(past_days, past_days + prediction_days):\n",
    "  X_test.append(inputs[i-past_days:i, 0])\n",
    "  y_test.append(inputs[i, 0])\n",
    "  \n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Cast\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "real_stock_price = dataset_test.Open.values\n",
    "plt.plot(real_stock_price, color='red', label='Real Stock Price')\n",
    "plt.plot(predicted_stock_price, color='blue', label='Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalueate for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error: {0:.2%}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
